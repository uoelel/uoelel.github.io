---
title: "Data analysis"
bibliography: references.bib
---

# Overview

::: {.callout-note icon="false"}
This page provides resources on how to **analyse measurements** (e.g. formant values, word counts, collexeme association metrics, semantic distance, etc.) you have obtained from your data.

For methods to **obtain specialised measurements and metrics** (e.g. formant values, word counts, collexeme association metrics, semantic distance, etc.), please check the (work-in-progress) [Measure](measures/overview.qmd) page.
:::

::: {.callout-warning icon="false"}
#### Disclaimer

This is not intended to be an exhaustive list, but rather a compendium of approaches and techniques that are currently gaining momentum across linguistic research.
For specialised approaches, we recommend to consult methods books written with specific audiences in mind.
Note that the methods presented here are general enough that they can be applied to a diverse range of data types.
:::

There are several resources you can use to teach yourself quantitative data analysis skills, depending on your level.
Don't forget to also check [Measures](measures/overview.qmd) and the [Skills](skills.qmd) page, for more general research-related skills.

# Beginners

::: {.callout-note icon="false"}
**Terminology in applied statistics tends to be unruly**, so that one same thing can have different names in different disciplines and vice versa one same name can mean different things.
We tried here to use a terminological set that is as "neutral" as possible, although we recognise the limitations of this approach.
Where relevant, we note alternative terms and their uses.
:::

The following resources are suitable for beginners who want to learn **quantitative data analysis from scratch**.

## Data wrangling and visualisation

> The simple graph has brought more information to the data analyst's mind than any other device.
>
> ---John Tukey

[Data wrangling](https://r4ds.had.co.nz/wrangle-intro.html) is about getting your data into a useful format for [visualisation](https://r4ds.had.co.nz/data-visualisation.html) and [modelling](https://r4ds.had.co.nz/model-intro.html).

The programming languages **Python and R** are two very common languages used for data analysis.

**Python** is a general-purpose programming language while **R** is specifically developed for statistical analysis and visualisation.
Most academic research uses R for data analysis, although Python is also employed especially for data processing.

If you want to teach yourself R, the following resources are an excellent place to start from:

-   The [R for Data Science](https://r4ds.had.co.nz) (R4DS) free online book is an excellent introduction to R and quantitative data analysis.

-   The [Data Visualisation Catalogue](https://datavizcatalogue.com/index.html) is a project developed by Severino Ribecca to create a (non-code-based) library of different information visualisation types.
    The website serves as a learning and inspirational resource for those working with data visualisation.

-   The workshop [intRo: Data Analysis with R](https://intro-rstats.github.io) introduces absolute beginners from the Humanities to R, quantitative data analysis and visualisation.
    Check out the videos on YouTube: [videos](https://youtube.com/playlist?list=PLiqxyEAY_i3nqPb2bp2zufXgPoFk8Tum9).
    You can find the materials and slides here: [materials](https://intro-rstats.github.io/intRo/) and [slides](https://intro-rstats.github.io/intro-slides/).

::: {.callout-note icon="false" collapse="true"}
#### Joining and pivoting data

Most data wrangling problems can be solved with the following sets of R tidyverse functions (of course if you use Python or other languages, feel free to use their equivalents): [mutating joins](https://dplyr.tidyverse.org/reference/mutate-joins.html) and [pivoting](https://tidyr.tidyverse.org/articles/pivot.html).

-   **Mutating joins** allow you to join two or more tibbles together so that you can include information from one tibble into another (for example, if you have participant info in one tibble and you want to join that with the main experiment results tibble).
    See here for a [visual representation of join operations](https://github.com/gadenbuie/tidyexplain#mutating-joins).

-   **Pivoting** makes it easy to transform a data table that has all the information you need but not in the right format (for example you have a two columns with the participant's scores at time point 1 and 2, but you want one column that has the time point and one that has the score).
    See here for a [visual representation of pivoting](https://github.com/gadenbuie/tidyexplain#tidy-data).
:::

## Statistical modelling

> The numbers have no way of speaking for themselves.
> We speak for them.
> We imbue them with meaning.
>
> ---Nate Silver, *The Signal and the Noise*

Statistics and statistical modelling are about finding meaning in the patterns that can be observed in the data.

-   [Statistics for Linguists: An introduction using R](https://discovered.ed.ac.uk/permalink/44UOE_INST/7g3mt6/alma9924344853202466) by Bodo Winter [@winter2020] is ideal both for absolute beginners and experienced researchers.
    It is packed with everything you need to successfully and effectively conduct statistical analyses.

-   [Statistical (Re)thinking](https://discovered.ed.ac.uk/permalink/44UOE_INST/110jsec/alma9924362502302466) by Richard McElreath is an excellent introduction for absolute beginners, by Richard McElreath, which covers a wide variety of linear models.
    It focusses on Bayesian inference and how this framework can help us directly answer research questions, assess evidence for different hypothesis, and quantify uncertainty.
    If you are familiar with the [tidyverse](https://www.tidyverse.org), the code from the *Statistical (Re)thinking* book has been translated into tidyverse by Solomon Kurz, and it can be accessed here: [Statistical rethinking with brms, ggplot2, and the tidyverse](https://bookdown.org/content/4857/).

-   [The Art of Statistics: Learning from data](https://www.penguin.co.uk/books/294/294857/learning-from-data/9780241258767.html) by David Spiegelhalter uses real-world examples to explain principles of data visualisation and analysis.
    It touches upon a wide range of topics and disciplines, from communicating proportions, to probability and Bayesian inference, making it a great complement to the other books and resources mentioned here.
    If you just wish to dip your toes in statistics without committing (yet) to learning how to do statistics, this book is for you.

-   [Linear models and linear mixed effects models in R with linguistic applications](https://arxiv.org/abs/1308.5499) by Bodo Winter is a short and intense tutorial on linear models [@winter2013].
    The first part introduces you to Gaussian linear models and the second part to Gaussian linear models that include random effects (variably called mixed, hierarchical, nested models).
    **Note** that Gaussian linear models are not appropriate for the data of most linguistic research and you will have to learn and use other types of linear models.

### Linear models

> One model to rule them all, one model to fit them,<br> One model to shrink them all, and in probability bind them;<br> In the Land of Inference where the distributions lie.
>
> ---The Statistical Hobbit.

**Linear models are a very flexible and relatively straightforward way to model and analyse quantitative data**.
They have gained momentum and are increasingly being adopted across the entire field of linguistics.

The main perk of learning linear models is that they **can be applied to many different types of data**, so that once you learn this approach you will be able to apply it to a lot of data analysis scenarios.

The resources mentioned above all focus on linear modelling, so whether you are just starting your statistical journey or you are an experienced researcher who wants to consolidate their understanding of linear modelling, those resources are right for you.

After you have learnt the basics, the [Linear models cheat-sheet](https://stefanocoretta.github.io/post/2021-08-21-regression-models-a-cheat-sheet/) can guide you through the process of choosing among the appropriate types of linear model depending on the nature of your data.
The post also lists tutorials on linear models that use other less common probability distribution families, like the beta, Poisson and ordinal.

Confused about all the model names?
Check out [this post](https://stefanocoretta.github.io/post/2022-07-22-one-thousand-and-one-names/) on how we don't really need to use all of those names: they are all linear models!

------------------------------------------------------------------------

# Intermediate

If you already have a basic understanding of quantitative data analysis, statistics and R, the following resources can help you develop your skills further.

## Likert scales

Likert-scale data are quite common in many fields of linguistics.
Likert scales are common especially in sociolinguistics work, for example in work that investigates attitudes: e.g. a 5-point scale "disagree, somewhat disagree, neutral, somewhat agree, agree".

Likert-scale data special because they are categorical *and* ordered.

Ordered data must be modelled using the appropriate distribution, namely the **cumulative distribution**.
**Ordinal linear models** are an extension of linear models that use the cumulative distribution to model ordinal data, like Likert-scale data.

For an excellent tutorial on how to fit ordinal linear models using brms applied to linguistics data, see @verissimo2021.
You should also check out the more general tutorial @burkner2019.
A recording of the [STeW](stew.qmd) workshop on ordinal regression model is also available [here](here).

## Count data (and corpus data)

Count data, like number of a particular construction in a corpus, number of interjection in a conversation, number of infant gestures, etc, should be modelled using a **Poisson distribution**.

See the tutorial by @winter2021.

## Dimensionality reduction

If your data is **highly dimensional**, i.e. **you have a lot of different variables** (some of which might even be correlated to each other), you can employ data dimensionality reduction techniques to "synthesise" all the variables into fewer variables that represent components, dimensions or clusters in the data.

These techniques can be used both (a) to find patterns or groupings in the data and to obtain measures that capture these patterns and groupings and (b) to simplify analyses from a set of 15/20 variables to 2/3 components or dimensions.

Note that once you have reduced your data to a few variables (components or dimensions), these can still be further analysed with the other techniques mentioned on this page.

-   A common reduction technique is **Principal Component Analysis** (PCA).
    This method combines all of your variables into a limited set of numeric *principal components*.
    The scores of the principal components capture variation in the data and can be used for further analysis.
    You can learn how to carry out a PCA with [this tutorial](http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/).
    Also check out Functional Principal Component Analysis below.

-   **Multiple Correspondence Analysis** (McA) is the discrete equivalent of PCA, i.e. it can be used with discrete/categorical variables.
    See [this tutorial](http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/114-mca-multiple-correspondence-analysis-in-r-essentials/) for an introduction.

-   Another dimensionality reduction technique is **Cluster Analysis** (CA, aka hierarchical clustering).
    [This tutorial](https://www.datanovia.com/en/blog/cluster-analysis-in-r-practical-guide/) guides you through a CA in R.

## Time series and coordinates

**Generalised Additive (Mixed) Models** (GAMMs) are a flexible extension of linear models that allows us to fit non-linear effects.
They are particularly useful with data that come from time series (e.g. f0 and formants, corpus occurrences across time, longitudinal data, etc.) and they can be employed with any kind of data that can be thought of as being represented on a coordinate space (e.g., geolocations, electroencephalographic (EEG) data, 3D tongue imaging, etc).

-   The tutorial by @soskuthy2017a is an excellent introduction to Generalised Additive Mixed Models (GAMMs).
    Also see @soskuthy2021.

-   Another excellent resource is @pedersen2019.
    In particular, [Figure 4](https://doi.org/10.7717/peerj.6876/fig-4) is a beautiful visual summary of how different types of trends and groupings can be modelled with GAMs.

-   The paper by @tamminga2016 advocates for the adoption of GAMMs to advance the use of naturalistic data for studying psycholinguistic questions about intra-speaker variation.

**Functional Principal Component Analysis** (FPCA) is another approach to modelling time-series data.

-   [Functional Data Analysis for Speech Research](https://github.com/uasolo/FPCA-phonetics-workshop) by Michele Gubian is a collection of workshop materials on Functional Data Analysis with a focus on speech research data.

## Bayesian inference

-   The overview by @etz2018, *How to become a Bayesian in eight easy steps: An annotated reading list*, is a good place to start from if you want to learn more about Bayesian statistics and inference.

-   For an more practice-oriented introduction, you should read [Statistical (Re)thinking](https://discovered.ed.ac.uk/permalink/44UOE_INST/110jsec/alma9924362502302466) (see above).

-   The [learnB4SS](https://learnb4ss.github.io) workshop is an introduction to **Bayesian analysis for the Speech Sciences**.
    It requires familiarity with linear models and Null Hypothesis Significance Testing.

------------------------------------------------------------------------

# Advanced

## Power analysis

Power analysis is a fundamental, although often neglected, step in Null Hypothesis Significance Testing (the statistical framework that returns *p*-values).
A power analysis is a method to estimate the minimum sample size necessary to detect a particular effect.
The statistical power of a test is the percentage of tests that correctly detect an effect when the effect indeed exists.
The recommended statistical power is 80% or greater.

Power analyses with linear models can become quite complex, especially if random effects are included.
Simulation is a way to simplify the calculations necessary to find the minimum sample size.
**The R package [simr](https://github.com/pitakakariki/simr) provides users with a set of functions to perform a power analysis with linear models using simulations.** You can find a [tutorial here](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12504).

If you are running **Bayesian linear models, you can check out this post on [Bayesian CrI-width power analysis](https://stefanocoretta.github.io/post/2022-04-05-bayesian-ci-width-power-analysis/)**.

## Multivariate linear models

[Estimating Multivariate Models with brms](https://cran.r-project.org/web/packages/brms/vignettes/brms_multivariate.html) by Paul Bürkner explains how to fit linear models with two or more outcome variables (i.e. multivariate models) using [brms](https://paul-buerkner.github.io/brms/).

## References {.unnumbered}
