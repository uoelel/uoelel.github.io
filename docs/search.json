[
  {
    "objectID": "support.html",
    "href": "support.html",
    "title": "Support",
    "section": "",
    "text": "There are different support channels for Hons/MSc/PhD students who are writing their dissertation, and researchers/staff who are seeking help with any aspect of quantitative data analysis."
  },
  {
    "objectID": "support.html#general-support",
    "href": "support.html#general-support",
    "title": "Support",
    "section": "1 General support",
    "text": "1 General support\nAs a general guideline aimed at students, supervisors, and staff, we have compiled a checklist that covers all the main aspects of quantitative analyses.\nYou can use the checklist as a template to think about your quantitative analysis plan at the early stages of your dissertation/research project.\nGo to checklist"
  },
  {
    "objectID": "support.html#honours-and-msc-students",
    "href": "support.html#honours-and-msc-students",
    "title": "Support",
    "section": "2 Honours and MSc students",
    "text": "2 Honours and MSc students\nIf you are a student writing your Honours or MSc dissertation and you are looking after help with your quantitative data analysis/statistics, you can book an appointment with Dr Stefano Coretta or Elizabeth Pankratz. Note that a university account is required for booking.\nBook with Stefano\n\nBook with Elizabeth"
  },
  {
    "objectID": "support.html#consultation-for-phd-students-postdocs-and-staff",
    "href": "support.html#consultation-for-phd-students-postdocs-and-staff",
    "title": "Support",
    "section": "3 Consultation for PhD students, postdocs and staff",
    "text": "3 Consultation for PhD students, postdocs and staff\nMany of the issues arising during the data analysis stage can be prevented by planning your analysis while designing the study and ahead of data collection. We strongly recommend you to work out your quantitative data analysis plan in details as early as possible. This expedient will save you time later on.\nPhD students, postdoc researchers and staff can get in touch with Dr Stefano Coretta, or directly book an appointment with Stefano via the link below (a university account is required).\nBook an appointment\n\nHelp is offered for the following areas:\n\nStudy design and operationalisation of research hypothesis.\nData wrangling and visualisation.\nStatistical modelling.\nResearch data management (including Data Management Plans).\nReporting.\nOpen Science practices."
  },
  {
    "objectID": "support.html#statistical-service-for-staff",
    "href": "support.html#statistical-service-for-staff",
    "title": "Support",
    "section": "4 Statistical service for staff",
    "text": "4 Statistical service for staff\nDr Stefano Coretta is available as a collaborator on projects or papers requiring the planning and execution of quantitative and statistical analyses.\nInterested postdocs or staff should contact Stefano for enquiries."
  },
  {
    "objectID": "stew.html",
    "href": "stew.html",
    "title": "LEL Statistical Training Workshops (STeW)",
    "section": "",
    "text": "The Linguistics and English Language Statistical Training Workshops (STeW) is a series of workshops that runs throughout the academic year and is aimed at students and staff who would like to advance their statistical knowledge and skills.\nOn this page you can find information on the current plan for the a.y. 2024-25. Attending requires you register and registration links are sent out via email at the beginning of each semester. Further workshops might be added later during the year.\nMost workshops are recorded and can be watched on the LEL Quantitative Methods channel on the UoE Media Hopper Create.\nIf you have any question, you can get in touch with Dr Stefano Coretta."
  },
  {
    "objectID": "stew.html#section",
    "href": "stew.html#section",
    "title": "LEL Statistical Training Workshops (STeW)",
    "section": "1 2024-25",
    "text": "1 2024-25\n\n1.1 Semester 1\n\n\n\n\n\n\nQuantitative Methods in LEL (QML)\n\n\n\n\n\n\nDate\nFull teaching term\n\n\nTime\nSee timetable\n\n\nLevel\nüü¢ Beginners\n\n\nDescription\nThis is the flagship stats course of the Linguistics and English Language department. It covers the basics of statistics in R and PhD Students are particularly encouraged to audit it.\n\n\nPrerequisites\nNone\n\n\nMaterials\nhttps://uoelel.github.io/qml/\n\n\n\n\n\n\n\n1.2 Semester 2\n\n\n\n\n\n\nOrdinal models for Likert/rating-scale data\n\n\n\n\n\n\nDate\nThursday 23 January 2025\n\n\nTime\n15.00‚Äì17.00\n\n\nLevel\nüî¥ Advanced\n\n\nDescription\nIn this workshop you will learn how to model data from Likert/rating scales, such as the ones commonly used in developmental and psycho- linguistics using ordinal models.\n\n\nPrerequisites\nNeed to be familiar with regression/linear models in R including models with random effects and binomial/Bernoulli (aka logistic) regressions. While a background in Bayesian inference is useful, the workshop will explain the basics of Bayesian inference in the context of ordinal models.\n\n\nMaterials\nhttps://stefanocoretta.github.io/learnOrd/\n\n\nRecording\nWatch\n\n\n\n\n\n\n\n\n\n\n\nProspective power analyses for frequentist regression models\n\n\n\n\n\n\nDate\nWednesday 26 March 2025\n\n\nTime\n15.00‚Äì17.00\n\n\nLevel\nüü† Intermediate\n\n\nDescription\nYou will learn how to perform prospective power analyses for regression/linear models fitted with the frequentist lme4 package. Power analyses are a necessary (albeit always neglected) component of frequentist analyses, since they help the researcher determine the required sample size. Much of the replicability crisis we are facing is due to lack of statistical power (aka low sample sizes) in virtually all published studies.\n\n\nPrerequisites\nNeed to be familiar with regression/linear models in R including models with random effects and binomial/Bernoulli (aka logistic) regressions.\n\n\nMaterials\nhttps://stefanocoretta.github.io/proPower/\n\n\nRecording\nWatch\n\n\n\n\n\n\n\n\n\n\n\nMindless statistics and the ‚Äúnull ritual‚Äù\n\n\n\n\n\n\nDate\nThursday 1 May 2025\n\n\nTime\n15.00‚Äì17.00\n\n\nLevel\nüü¢üü†üî¥ Any level\n\n\nDescription\nIn this workshop we will go through bad statistical practices that are common even among experienced researchers and how to avoid them. In particular, we will talk about the ‚Äúnull ritual‚Äù, which is a invalid approach to frequentist statistics commonly used in research. Most of the content of the workshop will be drawn from Gigerenzer‚Äôs Mindless Statistics paper.\n\n\nPrerequisites\nSome familiarity with frequentist statistics is useful but not necessary. This workshop is thought for both beginners and experienced researchers or students, whether they normally use quantitative, qualitative or mixed methods.\n\n\nMaterials\nhttps://stefanocoretta.github.io/null-ritual/"
  },
  {
    "objectID": "stew.html#previously",
    "href": "stew.html#previously",
    "title": "LEL Statistical Training Workshops (STeW)",
    "section": "2 Previously",
    "text": "2 Previously\n\n\n\n\n\n\nintRo: Data visualisation with R\n\n\n\n\n\n\nDate\n18 December 2022\n\n\nLevel\nüü¢ Beginners\n\n\nDescription\nThis is an introduction to R and data visualisation for absolute beginners. If you want to learn R for the first time, this is a good start.\n\n\nPrerequisites\nNone.\n\n\nMaterials\nhttps://intro-rstats.github.io\n\n\nRecording\nDay 1, Day 2\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Bayesian Linear Models\n\n\n\n\n\n\nDate\nThursday 26 October 2023\n\n\nTime\n15.00‚Äì17.00\n\n\nLevel\nüü† Intermediate\n\n\nMaterials\nhttps://stefanocoretta.github.io/learnBayes/\n\n\nRecording\nWatch\n\n\n\n\n\n\n\n\n\n\n\nIncluding random effects in Bayesian Linear models with brm()\n\n\n\n\n\n\nDate\nThursday 16 May 2024\n\n\nTime\n15.00‚Äì17.00\n\n\nLevel\nüü† Intermediate\n\n\nDescription\nThis workshop is targeted especially to MSc students who have attended the QML course.\n\n\nPrerequisites\nAnybody who has attended the previous Bayesian workshops or who has experience with basic linear models with brm() is welcome to join.\n\n\nMaterials\nhttps://uoelel.github.io/brm-group/\n\n\nRecording\nWatch\n\n\n\n\n\n\n\n\n\n\n\nGeneralised Additive Mixed Models (GAMMs)\n\n\n\n\n\n\nDate\nThursday 18 April 2024\n\n\nTime\n15.00‚Äì17.00\n\n\nLevel\nüü† Intermediate\n\n\nDescription\nGAMMs are useful to analyse historical data, time-series data or data that could have non-linear effects (like language development data).\n\n\nPrerequisites\nIt assumes some familiarity with linear models fitted with lmer()/glmer() or brm().\n\n\nMaterials\nhttps://stefanocoretta.github.io/learnGAM/\n\n\nRecording\nWatch (recording of previous run of the workshop)\n\n\n\n\n\n\n\n\n\n\n\nBayesian Priors in Linear Models\n\n\n\n\n\n\nDate\nThursday 22 February 2024\n\n\nTime\n15.00‚Äì17.00\n\n\nLevel\nüî¥ Advanced\n\n\nPrerequisites\nNeed to be familiar with Bayesian Linear Models including Random Effects and/or have attended the Introduction to Bayesian Linear Models workshop. This is not suitable for students who have only attended the QML course (see below for dedicated workshop).\n\n\nMaterials\nhttps://stefanocoretta.github.io/learnBayes/ (Session 2)\n\n\nRecording\nUnfortunately I forgot to record this, but you can watch the following videos from another workshop to learn about priors (these videos go into much more detail!).\n\nlearnB4SS ‚Äî 03 Application to Regression II: Priors and Bayesian Updating (Part I)\nlearnB4SS ‚Äî 03 Application to Regression II: Priors and Bayesian Updating (Part II)"
  },
  {
    "objectID": "skills.html",
    "href": "skills.html",
    "title": "Research Skills",
    "section": "",
    "text": "Best practices for researchers compiled by the Open Science Foundation. From file management to publishing research output.\n\n\n\nSeven Easy Steps to Open Science: An Annotated Reading List will provide you with a great overview of Open Science and how to implement Open Science practices in your own research.\nThe first thing you can do to make your research open is to share the data and analysis code in public repositories, like the Open Science Framework.\n\n\n\nLearn Version Control with Git is a gentle introduction to version control using the software git. With git you can easily keep ‚Äúsnapshots‚Äù of your code, papers or dissertation, and track the full history of changes so that you can easily roll back to a previous state if necessary."
  },
  {
    "objectID": "skills.html#research-management",
    "href": "skills.html#research-management",
    "title": "Research Skills",
    "section": "",
    "text": "Best practices for researchers compiled by the Open Science Foundation. From file management to publishing research output.\n\n\n\nSeven Easy Steps to Open Science: An Annotated Reading List will provide you with a great overview of Open Science and how to implement Open Science practices in your own research.\nThe first thing you can do to make your research open is to share the data and analysis code in public repositories, like the Open Science Framework.\n\n\n\nLearn Version Control with Git is a gentle introduction to version control using the software git. With git you can easily keep ‚Äúsnapshots‚Äù of your code, papers or dissertation, and track the full history of changes so that you can easily roll back to a previous state if necessary."
  },
  {
    "objectID": "skills.html#write-up",
    "href": "skills.html#write-up",
    "title": "Research Skills",
    "section": "2 Write up",
    "text": "2 Write up\nFor help on Writing skills, please check the Writing Skills Centre [login required].\n\n2.1 Dynamic documents with Quarto and Rmarkdown\nWrite dynamic documents, from papers to dissertations, with Quarto or Rmarkdown.\nDynamic documents are documents that mix plain text and code, so that you can embed your analyses straight into the document.\nCode output like model summaries and plots are directly rendered within the document, and you don‚Äôt have to include them manually. The benefit is that if your data or analysis has changed, you can simply render the document again and everything will be up-to-date!\n\n\n\n\n\n\nQuarto or Rmarkdown?\n\n\n\nQuarto is the official successor of Rmarkdown and it is heavily based on Rmarkdown. In fact, Quarto is 100% backward compatible with Rmarkdown. Rmarkdown will not go away!\n\n\n\n\n2.2 Write with LaTeX\nLearn LaTeX in 30 minutes and focus on writing content rather than formatting it."
  },
  {
    "objectID": "skills.html#other-resources",
    "href": "skills.html#other-resources",
    "title": "Research Skills",
    "section": "3 Other resources",
    "text": "3 Other resources\nYou should also check out the Practical Research Skills website. Although it focusses on psychological research, most of the methods and concepts also apply to linguistic research."
  },
  {
    "objectID": "measures/overview.html",
    "href": "measures/overview.html",
    "title": "Measures and metrics",
    "section": "",
    "text": "Warning\n\n\n\nWork in progress‚Ä¶\n\n\nWe are collecting commonly used measurements and metrics for different branches of linguistics and types of data.\nExplore the available pages using the drop-down menu under ‚ÄúMeasures‚Äù."
  },
  {
    "objectID": "extra/variable-types.html",
    "href": "extra/variable-types.html",
    "title": "Variable types",
    "section": "",
    "text": "0.1 Continuous variables\n\nThe variable can take on any positive and negative real number, including 0: Gaussian (aka normal) distribution.\n\nThere are very few truly Gaussian variables, although in some cases one can speak of ‚Äúapproximate‚Äù or ‚Äúassumed‚Äù normality.\nThis family is fitted by default in lm(), lme4::lmer() and brms::brm().\n\nThe variable can take on any positive number only: Log-normal distribution.\n\nDuration of segments, words, pauses, etc, are known to be log-normally distributed.\nMeasurements taken in Hz (like f0, formants, centre of gravity, ‚Ä¶) could be considered to be log-normal.\nThere other families that could potentially be used depending on the nature of the variable: exponential-Gaussian (reaction times), gamma, ‚Ä¶\n\nThe variable can take on any number between 0 and 1, but not 0 nor 1: Beta distribution.\n\nProportions fall into this category (for example proportion of voicing within closure), although 0 and 1 are not allowed in the beta distribution.\n\nThe variable can take on any number between 0 and 1, including 0 or 0 and 1: Zero-inflated or Zero/one-inflated beta (ZOIB) distribution.\n\nIf the proportion data includes many 0s and 1s, then this is the ideal distribution to use. ZOIB distributions are somewhat more difficult to fit than a simple beta distribution, so a common practice is to transform the data so that it doesn‚Äôt include 0s nor 1s (this can be achieved using different techniques, some better than others).\n\n\n\n\n0.2 Discrete variables\n\nThe variable is dichotomous, i.e.¬†it can take one of two levels: Bernoulli distribution.\n\nCategorical outcome variables like yes/no, correct/incorrect, voiced/voiceless, follow this distribution.\nThis family is fitted by default when you run glm(family = binomial), aka ‚Äúlogistic regression‚Äù or ‚Äúbinomial regression‚Äù.\n\nThe variable is counts: Poisson distribution.\n\nCounts of words, segments, gestures, f0 peaks, ‚Ä¶\n\nThe variable is a scale: ordinal linear model.\n\nLikert scales and ratings, language attitude questionnaires.\nOrdinal linear models, a.k.a. ordinal logistic regression, can be fitted with the ordinal and the brms package."
  },
  {
    "objectID": "extra/falsifiable.html",
    "href": "extra/falsifiable.html",
    "title": "Falsifiability",
    "section": "",
    "text": "A statement is falsifiable if it can be contradicted by an observation. If such observation is impossible to make with current technology, falsifiability is not achieved.\n\nFrom Seven examples of falsifiability by John Spacey."
  },
  {
    "objectID": "extra/falsifiable.html#definition",
    "href": "extra/falsifiable.html#definition",
    "title": "Falsifiability",
    "section": "",
    "text": "A statement is falsifiable if it can be contradicted by an observation. If such observation is impossible to make with current technology, falsifiability is not achieved.\n\nFrom Seven examples of falsifiability by John Spacey."
  },
  {
    "objectID": "extra/falsifiable.html#falsifiable-statements",
    "href": "extra/falsifiable.html#falsifiable-statements",
    "title": "Falsifiability",
    "section": "2 Falsifiable statements",
    "text": "2 Falsifiable statements\n\n‚ÄúLife only exists on Earth.‚Äù (it would be falsified by the observation of life somewhere else).\n‚ÄúIf there is a 1st person exclusive dual, then there is also a 1st person inclusive dual.‚Äù [Universal 1871] (it would be falsified by the observation of languages with a 1st person exclusive dual but without the incluve alternative).\n‚ÄúInfants start uttering full sentences only after their 12th month of life.‚Äù (it would be falsified by the observation of infants uttering full sentences before their 12th month of life)."
  },
  {
    "objectID": "extra/falsifiable.html#non-falsifiable-statements",
    "href": "extra/falsifiable.html#non-falsifiable-statements",
    "title": "Falsifiability",
    "section": "3 Non-falsifiable statements",
    "text": "3 Non-falsifiable statements\n\n‚ÄúLife might exist outside of the Solar system.‚Äù (if we observe life outside the Solar system or we don‚Äôt, the statement is still true).\n‚ÄúLanguages with a 1st person inclusive dual can have a 1st person exclusive dual.‚Äù (whether we observe a language with both 1st inclusive and exclusive dual or not, the statement is still true.)"
  },
  {
    "objectID": "courses.html",
    "href": "courses.html",
    "title": "Courses",
    "section": "",
    "text": "LEL students can take the following courses that focus on research methods and data analysis. See DRPS for course info.\n\nComputer Programming for Speech and Language Processing.\nData Analysis for LEL (UG).\nDiscourse Analysis.\nIntroduction to Discourse Analysis (PG).\nLinguistic Fieldwork and Language Description (UG/PG).\nMethods in Theoretical Linguistics (UG).\nOnline Experiments for Language Scientists (UG/PG).\nQuantitative Methods for LEL (PG).\nResearch Ethics Training in Linguistics & English Language (UG/PG).\nResearch Methods in Developmental Linguistics (PG).\nSociolinguistic Research Design (PG)."
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Data analysis",
    "section": "",
    "text": "Note\n\n\n\nThis page provides resources on how to analyse measurements (e.g.¬†formant values, word counts, collexeme association metrics, semantic distance, etc.) you have obtained from your data.\nFor methods to obtain specialised measurements and metrics (e.g.¬†formant values, word counts, collexeme association metrics, semantic distance, etc.), please check the (work-in-progress) Measure page.\n\n\n\n\n\n\n\n\nDisclaimer\n\n\n\nThis is not intended to be an exhaustive list, but rather a compendium of approaches and techniques that are currently gaining momentum across linguistic research. For specialised approaches, we recommend to consult methods books written with specific audiences in mind. Note that the methods presented here are general enough that they can be applied to a diverse range of data types.\n\n\nThere are several resources you can use to teach yourself quantitative data analysis skills, depending on your level. Don‚Äôt forget to also check Measures and the Skills page, for more general research-related skills."
  },
  {
    "objectID": "analysis.html#data-wrangling-and-visualisation",
    "href": "analysis.html#data-wrangling-and-visualisation",
    "title": "Data analysis",
    "section": "2.1 Data wrangling and visualisation",
    "text": "2.1 Data wrangling and visualisation\n\nThe simple graph has brought more information to the data analyst‚Äôs mind than any other device.\n‚ÄîJohn Tukey\n\nData wrangling is about getting your data into a useful format for visualisation and modelling.\nThe programming languages Python and R are two very common languages used for data analysis.\nPython is a general-purpose programming language while R is specifically developed for statistical analysis and visualisation. Most academic research uses R for data analysis, although Python is also employed especially for data processing.\nIf you want to teach yourself R, the following resources are an excellent place to start from:\n\nThe R for Data Science (R4DS) free online book is an excellent introduction to R and quantitative data analysis.\nThe Data Visualisation Catalogue is a project developed by Severino Ribecca to create a (non-code-based) library of different information visualisation types. The website serves as a learning and inspirational resource for those working with data visualisation.\nThe workshop intRo: Data Analysis with R introduces absolute beginners from the Humanities to R, quantitative data analysis and visualisation. Check out the videos on YouTube: videos. You can find the materials and slides here: materials and slides.\n\n\n\n\n\n\n\nJoining and pivoting data\n\n\n\n\n\nMost data wrangling problems can be solved with the following sets of R tidyverse functions (of course if you use Python or other languages, feel free to use their equivalents): mutating joins and pivoting.\n\nMutating joins allow you to join two or more tibbles together so that you can include information from one tibble into another (for example, if you have participant info in one tibble and you want to join that with the main experiment results tibble). See here for a visual representation of join operations.\nPivoting makes it easy to transform a data table that has all the information you need but not in the right format (for example you have a two columns with the participant‚Äôs scores at time point 1 and 2, but you want one column that has the time point and one that has the score). See here for a visual representation of pivoting."
  },
  {
    "objectID": "analysis.html#statistical-modelling",
    "href": "analysis.html#statistical-modelling",
    "title": "Data analysis",
    "section": "2.2 Statistical modelling",
    "text": "2.2 Statistical modelling\n\nThe numbers have no way of speaking for themselves. We speak for them. We imbue them with meaning.\n‚ÄîNate Silver, The Signal and the Noise\n\nStatistics and statistical modelling are about finding meaning in the patterns that can be observed in the data.\n\nStatistics for Linguists: An introduction using R by Bodo Winter (Winter 2020) is ideal both for absolute beginners and experienced researchers. It is packed with everything you need to successfully and effectively conduct statistical analyses.\nStatistical (Re)thinking by Richard McElreath is an excellent introduction for absolute beginners, by Richard McElreath, which covers a wide variety of linear models. It focusses on Bayesian inference and how this framework can help us directly answer research questions, assess evidence for different hypothesis, and quantify uncertainty. If you are familiar with the tidyverse, the code from the Statistical (Re)thinking book has been translated into tidyverse by Solomon Kurz, and it can be accessed here: Statistical rethinking with brms, ggplot2, and the tidyverse.\nThe Art of Statistics: Learning from data by David Spiegelhalter uses real-world examples to explain principles of data visualisation and analysis. It touches upon a wide range of topics and disciplines, from communicating proportions, to probability and Bayesian inference, making it a great complement to the other books and resources mentioned here. If you just wish to dip your toes in statistics without committing (yet) to learning how to do statistics, this book is for you.\nLinear models and linear mixed effects models in R with linguistic applications by Bodo Winter is a short and intense tutorial on linear models (Winter 2013). The first part introduces you to Gaussian linear models and the second part to Gaussian linear models that include random effects (variably called mixed, hierarchical, nested models). Note that Gaussian linear models are not appropriate for the data of most linguistic research and you will have to learn and use other types of linear models.\n\n\n2.2.1 Linear models\n\nOne model to rule them all, one model to fit them, One model to shrink them all, and in probability bind them; In the Land of Inference where the distributions lie.\n‚ÄîThe Statistical Hobbit.\n\nLinear models are a very flexible and relatively straightforward way to model and analyse quantitative data. They have gained momentum and are increasingly being adopted across the entire field of linguistics.\nThe main perk of learning linear models is that they can be applied to many different types of data, so that once you learn this approach you will be able to apply it to a lot of data analysis scenarios.\nThe resources mentioned above all focus on linear modelling, so whether you are just starting your statistical journey or you are an experienced researcher who wants to consolidate their understanding of linear modelling, those resources are right for you.\nAfter you have learnt the basics, the Linear models cheat-sheet can guide you through the process of choosing among the appropriate types of linear model depending on the nature of your data. The post also lists tutorials on linear models that use other less common probability distribution families, like the beta, Poisson and ordinal.\nConfused about all the model names? Check out this post on how we don‚Äôt really need to use all of those names: they are all linear models!"
  },
  {
    "objectID": "analysis.html#likert-scales",
    "href": "analysis.html#likert-scales",
    "title": "Data analysis",
    "section": "3.1 Likert scales",
    "text": "3.1 Likert scales\nLikert-scale data are quite common in many fields of linguistics. Likert scales are common especially in sociolinguistics work, for example in work that investigates attitudes: e.g.¬†a 5-point scale ‚Äúdisagree, somewhat disagree, neutral, somewhat agree, agree‚Äù.\nLikert-scale data special because they are categorical and ordered.\nOrdered data must be modelled using the appropriate distribution, namely the cumulative distribution. Ordinal linear models are an extension of linear models that use the cumulative distribution to model ordinal data, like Likert-scale data.\nFor an excellent tutorial on how to fit ordinal linear models using brms applied to linguistics data, see Verissimo (2021). You should also check out the more general tutorial B√ºrkner and Vuorre (2019). A recording of the STeW workshop on ordinal regression model is also available here."
  },
  {
    "objectID": "analysis.html#count-data-and-corpus-data",
    "href": "analysis.html#count-data-and-corpus-data",
    "title": "Data analysis",
    "section": "3.2 Count data (and corpus data)",
    "text": "3.2 Count data (and corpus data)\nCount data, like number of a particular construction in a corpus, number of interjection in a conversation, number of infant gestures, etc, should be modelled using a Poisson distribution.\nSee the tutorial by Winter and B√ºrkner (2021)."
  },
  {
    "objectID": "analysis.html#dimensionality-reduction",
    "href": "analysis.html#dimensionality-reduction",
    "title": "Data analysis",
    "section": "3.3 Dimensionality reduction",
    "text": "3.3 Dimensionality reduction\nIf your data is highly dimensional, i.e.¬†you have a lot of different variables (some of which might even be correlated to each other), you can employ data dimensionality reduction techniques to ‚Äúsynthesise‚Äù all the variables into fewer variables that represent components, dimensions or clusters in the data.\nThese techniques can be used both (a) to find patterns or groupings in the data and to obtain measures that capture these patterns and groupings and (b) to simplify analyses from a set of 15/20 variables to 2/3 components or dimensions.\nNote that once you have reduced your data to a few variables (components or dimensions), these can still be further analysed with the other techniques mentioned on this page.\n\nA common reduction technique is Principal Component Analysis (PCA). This method combines all of your variables into a limited set of numeric principal components. The scores of the principal components capture variation in the data and can be used for further analysis. You can learn how to carry out a PCA with this tutorial. Also check out Functional Principal Component Analysis below.\nMultiple Correspondence Analysis (McA) is the discrete equivalent of PCA, i.e.¬†it can be used with discrete/categorical variables. See this tutorial for an introduction.\nAnother dimensionality reduction technique is Cluster Analysis (CA, aka hierarchical clustering). This tutorial guides you through a CA in R."
  },
  {
    "objectID": "analysis.html#time-series-and-coordinates",
    "href": "analysis.html#time-series-and-coordinates",
    "title": "Data analysis",
    "section": "3.4 Time series and coordinates",
    "text": "3.4 Time series and coordinates\nGeneralised Additive (Mixed) Models (GAMMs) are a flexible extension of linear models that allows us to fit non-linear effects. They are particularly useful with data that come from time series (e.g.¬†f0 and formants, corpus occurrences across time, longitudinal data, etc.) and they can be employed with any kind of data that can be thought of as being represented on a coordinate space (e.g., geolocations, electroencephalographic (EEG) data, 3D tongue imaging, etc).\n\nThe tutorial by S√≥skuthy (2017) is an excellent introduction to Generalised Additive Mixed Models (GAMMs). Also see S√≥skuthy (2021).\nAnother excellent resource is Pedersen et al. (2019). In particular, Figure 4 is a beautiful visual summary of how different types of trends and groupings can be modelled with GAMs.\nThe paper by Tamminga, Ahern, and Ecay (2016) advocates for the adoption of GAMMs to advance the use of naturalistic data for studying psycholinguistic questions about intra-speaker variation.\n\nFunctional Principal Component Analysis (FPCA) is another approach to modelling time-series data.\n\nFunctional Data Analysis for Speech Research by Michele Gubian is a collection of workshop materials on Functional Data Analysis with a focus on speech research data."
  },
  {
    "objectID": "analysis.html#bayesian-inference",
    "href": "analysis.html#bayesian-inference",
    "title": "Data analysis",
    "section": "3.5 Bayesian inference",
    "text": "3.5 Bayesian inference\n\nThe overview by Etz et al. (2018), How to become a Bayesian in eight easy steps: An annotated reading list, is a good place to start from if you want to learn more about Bayesian statistics and inference.\nFor an more practice-oriented introduction, you should read Statistical (Re)thinking (see above).\nThe learnB4SS workshop is an introduction to Bayesian analysis for the Speech Sciences. It requires familiarity with linear models and Null Hypothesis Significance Testing."
  },
  {
    "objectID": "analysis.html#power-analysis",
    "href": "analysis.html#power-analysis",
    "title": "Data analysis",
    "section": "4.1 Power analysis",
    "text": "4.1 Power analysis\nPower analysis is a fundamental, although often neglected, step in Null Hypothesis Significance Testing (the statistical framework that returns p-values). A power analysis is a method to estimate the minimum sample size necessary to detect a particular effect. The statistical power of a test is the percentage of tests that correctly detect an effect when the effect indeed exists. The recommended statistical power is 80% or greater.\nPower analyses with linear models can become quite complex, especially if random effects are included. Simulation is a way to simplify the calculations necessary to find the minimum sample size. The R package simr provides users with a set of functions to perform a power analysis with linear models using simulations. You can find a tutorial here.\nIf you are running Bayesian linear models, you can check out this post on Bayesian CrI-width power analysis."
  },
  {
    "objectID": "analysis.html#multivariate-linear-models",
    "href": "analysis.html#multivariate-linear-models",
    "title": "Data analysis",
    "section": "4.2 Multivariate linear models",
    "text": "4.2 Multivariate linear models\nEstimating Multivariate Models with brms by Paul B√ºrkner explains how to fit linear models with two or more outcome variables (i.e.¬†multivariate models) using brms."
  },
  {
    "objectID": "extra/analysis-checklist.html",
    "href": "extra/analysis-checklist.html",
    "title": "Quantitative analysis plan",
    "section": "",
    "text": "Warning\n\n\n\nThe information given on this page concerns only students who will carry out quantitative analyses (so it does not apply to students doing qualitative analyses).\nMake sure you plan ahead and think about the quantitative analysis of the data you will collect for your study. The best time to do so is when you are planning your study/project. Do not wait until after you started collecting data."
  },
  {
    "objectID": "extra/analysis-checklist.html#the-checklist",
    "href": "extra/analysis-checklist.html#the-checklist",
    "title": "Quantitative analysis plan",
    "section": "1 The checklist",
    "text": "1 The checklist\nDuring the early stages of your project, bring this list of questions to your supervisor and discuss them. If, after talking to your supervisor, you are still unsure about any of these points, please book an appointment with Stefano.\n\nCan you clearly state, in simple language, what the research question(s) of your project is/are?\nIf you are testing a specific hypothesis, did you formulate it so that it is a falsifiable statement? Note that it is OK if you don‚Äôt or cannot formulate a hypothesis! As long as you have one meaningful research question, you are all set.\nHave you clearly defined the concepts/objects of study in your research question and hypothesis?\nHave you operationalised the concepts/objects of study into clearly measurable variables?\nWhich are your outcome (dependent) variable(s) and your predictor (independent) variable(s)?\nYour outcome variable(s) is of which type?\nWhich statistical model or test will you use to answer your research question or test your research hypothesis? Note that you should consider statistical modelling or testing only if you have attended a course on statistics/quantitative methods.\nHave you specified and justified the minimal sample size you would need to obtain the required estimate precision or statistical power (if you will be doing statistical modelling or testing)?"
  },
  {
    "objectID": "extra/regression-sheet.html",
    "href": "extra/regression-sheet.html",
    "title": "Regression models: a cheat-sheet",
    "section": "",
    "text": "Regression models (aka linear regression models, linear models, generalised linear models) are a group of statistical models based on the simple idea that we can predict an outcome variable \\(Y\\) based on a function \\(f(X)\\).\nThe ‚Äúsimplest‚Äù linear model is the formula of a line:1\n\\[\ny = \\alpha + \\beta x\n\\]\nwhere \\(\\alpha\\) is the intercept of the line and \\(\\beta\\) the slope (aka gradient).\nThe principles behind this formula can be extended to represent virtually any other model, independent of the nature of the outcome variable(s) (\\(y\\)), the predictor(s), the types of relationship between outcome and predictor, and so on. This means that if you master the principles of regression models, then you can virtually fit any kind of data using regression models. You can bid farewell to ANOVAs, \\(t\\)-tests, \\(\\chi^2\\)-tests, and what not. In fact, these can all be thought of as specific cases of regression models. It just so happens that they got themselves a specific name. But the underlying mechanics is the same.\nSame goes with ‚Äúlinear models‚Äù, ‚Äúlogistic regression‚Äù, ‚Äúgeneralised linear models‚Äù, ‚Äúmixed-effects regression‚Äù and so on. These are all regression models, so they all follow the same principles. And again, the fact that they got specific name is a historical ‚Äúaccident‚Äù. Understanding that these named models are in fact all regression models gives you super powers you can use on data (Sauron would be so jealous):\n\nOne model to rule them all, one model to fit them,\nOne model to shrink them all, and in probability bind them;\nIn the Land of Inference where the distributions lie.\n\nEhm‚Ä¶ perhaps this is not gonna win a poetry context, but‚Ä¶ the message is that with a single tool, i.e.¬†regression models, you can go a long way!\nEach of the following sections asks you about the nature of your data and/or experimental design. By answering each, you will find out which ‚Äúpieces‚Äù you need to add to your model structure."
  },
  {
    "objectID": "extra/regression-sheet.html#one-model-to-rule-them-all",
    "href": "extra/regression-sheet.html#one-model-to-rule-them-all",
    "title": "Regression models: a cheat-sheet",
    "section": "",
    "text": "Regression models (aka linear regression models, linear models, generalised linear models) are a group of statistical models based on the simple idea that we can predict an outcome variable \\(Y\\) based on a function \\(f(X)\\).\nThe ‚Äúsimplest‚Äù linear model is the formula of a line:1\n\\[\ny = \\alpha + \\beta x\n\\]\nwhere \\(\\alpha\\) is the intercept of the line and \\(\\beta\\) the slope (aka gradient).\nThe principles behind this formula can be extended to represent virtually any other model, independent of the nature of the outcome variable(s) (\\(y\\)), the predictor(s), the types of relationship between outcome and predictor, and so on. This means that if you master the principles of regression models, then you can virtually fit any kind of data using regression models. You can bid farewell to ANOVAs, \\(t\\)-tests, \\(\\chi^2\\)-tests, and what not. In fact, these can all be thought of as specific cases of regression models. It just so happens that they got themselves a specific name. But the underlying mechanics is the same.\nSame goes with ‚Äúlinear models‚Äù, ‚Äúlogistic regression‚Äù, ‚Äúgeneralised linear models‚Äù, ‚Äúmixed-effects regression‚Äù and so on. These are all regression models, so they all follow the same principles. And again, the fact that they got specific name is a historical ‚Äúaccident‚Äù. Understanding that these named models are in fact all regression models gives you super powers you can use on data (Sauron would be so jealous):\n\nOne model to rule them all, one model to fit them,\nOne model to shrink them all, and in probability bind them;\nIn the Land of Inference where the distributions lie.\n\nEhm‚Ä¶ perhaps this is not gonna win a poetry context, but‚Ä¶ the message is that with a single tool, i.e.¬†regression models, you can go a long way!\nEach of the following sections asks you about the nature of your data and/or experimental design. By answering each, you will find out which ‚Äúpieces‚Äù you need to add to your model structure."
  },
  {
    "objectID": "extra/regression-sheet.html#step-0-number-of-outcome-variables",
    "href": "extra/regression-sheet.html#step-0-number-of-outcome-variables",
    "title": "Regression models: a cheat-sheet",
    "section": "2 Step 0: Number of outcome variables",
    "text": "2 Step 0: Number of outcome variables\nWe will get back to this step at the end of this post, since it makes things a bit more complex."
  },
  {
    "objectID": "extra/regression-sheet.html#sec-outcome",
    "href": "extra/regression-sheet.html#sec-outcome",
    "title": "Regression models: a cheat-sheet",
    "section": "3 Step 1: Choose a distribution for your outcome variable",
    "text": "3 Step 1: Choose a distribution for your outcome variable\nThe first step towards building a regression model is to choose the family of distributions you believe the outcome variable belongs to. You can start by answering the following question.\n\n\n\n\n\n\nQuestion 1\n\n\n\nIs the outcome variable continuous or discrete?\n\n\nDepending on the answer, check out Section¬†3.1 or Section¬†3.2.\n\n3.1 Continuous outcome variable\n\nThe variable can take on any positive and negative real number, including 0: Gaussian (aka normal) distribution.\n\nThere are very few truly Gaussian variables, although in some cases one can speak of ‚Äúapproximate‚Äù or ‚Äúassumed‚Äù normality.\nThis family is fitted by default in lm(), lme4::lmer() and brms::brm(). You can explicitly select the family with family = gaussian.\n\nThe variable can take on any positive real number only: Log-normal distribution.\n\nDuration of segments, words, pauses, etc, are known to be log-normally distributed.\nReaction times can be modelled with a log-normal distribution.\nMeasurements taken in Hz (like f0, formants, centre of gravity, ‚Ä¶) could be considered to be log-normal.\nThere other families that could potentially be used depending on the nature of the variable: exponential-Gaussian (reaction times), gamma, ‚Ä¶\nFit a log-normal model with brms::brm(..., family = lognormal).\n\nThe variable can take on any real number between 0 and 1, but not 0 nor 1: Beta distribution.\n\nProportions fall into this category (for example proportion of voicing within closure), although 0 and 1 are not allowed in the beta distribution.\nFit a beta model with brms::brm(..., family = Beta).\nCheck this tutorial: Coretta and B√ºrkner (n.d.).\n\nThe variable can take on any real number between 0 and 1, including 0 or 0 and 1: Zero-inflated or Zero/one-inflated beta (ZOIB) distribution.\n\nIf the proportion data includes many 0s and 1s, then this is the ideal distribution to use. ZOIB distributions are somewhat more difficult to fit than a simple beta distribution, so a common practice is to transform the data so that it doesn‚Äôt include 0s nor 1s (this can be achieved using different techniques, some better than others).\nFit a ZOIB model with brms::brm(..., family = zero_one_inflated_beta.\nCheck this tutorial: Heiss (2021).\n\n\n\n\n3.2 Discrete outcome variable\n\nThe variable is dichotomous, i.e.¬†it can take one of two levels: Bernoulli distribution.\n\nCategorical outcome variables like yes/no, correct/incorrect, voiced/voiceless, follow this distribution.\nThis family is fitted by default when you run glm(..., family = binomial), aka ‚Äúlogistic regression‚Äù or ‚Äúbinomial regression‚Äù or with brms::brm(..., family = bernoulli).2\n\nThe variable is counts: Poisson distribution.\n\nCounts of words, segments, gestures, f0 peaks, ‚Ä¶\nCheck out this tutorial: Winter and B√ºrkner (2021).\nFit a Poisson model with brms::brm(..., family = poisson).\nSometimes a negative binomial distribution is preferable, if the count data is dispersed. Fit this model with brms::brm(..., family = negbinomial).\n\nThe variable is a scale: ordinal linear model.\n\nLikert scales and ratings, language attitude questionnaires.\nFit ordinal regression models (aka ordinal logistic regression) with brms::brm(..., family = cumulative).\nSee these tutorials: Verissimo (2021), B√ºrkner and Vuorre (2019)."
  },
  {
    "objectID": "extra/regression-sheet.html#step-2-are-there-hierarchical-groupings-andor-repeated-measures",
    "href": "extra/regression-sheet.html#step-2-are-there-hierarchical-groupings-andor-repeated-measures",
    "title": "Regression models: a cheat-sheet",
    "section": "4 Step 2: Are there hierarchical groupings and/or repeated measures?",
    "text": "4 Step 2: Are there hierarchical groupings and/or repeated measures?\nThe second step is to ensure that, if the data is structured hierarchically or repeated measures were taken, this is taken into account in the model. Here is where so-called varying terms (aka random effects, group-level effects/terms) come in (Gelman 2005). Models that include random effects/group-level terms are called: random-effects models, mixed-effects models, hierarchical models, nested models, multilevel models. These terms are for all intents and purposes equivalent (it just happens that different traditions use different terms).\nAs an example, let‚Äôs assume you asked a number of participants to read a list of words and each word was repeated 5 times by each participant. You then took f0 measurements from the stressed vowel of each word, of each repetition. Now, the data has a ‚Äúhierarchical‚Äù structure to it:\n\nFirst, observations are grouped by participant (some observations belong to one participant and others to another and so on).\nSecond, observations are grouped by word (some observations belong to one word and others to another and so on).\nThird, within the observations of each word, some belong to the same participant (or, from a different perspective, within the observations of each participant, some belong to the same word).\n\nThe presence of ‚Äúlevels‚Äù within the data (whether they come from natural groupings like participant or word, or from repeated measures) breaks one of the assumptions of regression models: that each observation must be independent. This is why you must include varying terms in the regression model, to account for this structure (and now you see why they are called hierarchical and multilevel models). If you don‚Äôt include any varying term, your model will expect that each observation is independent and hence it will underestimate variance and return unreliable results.\nIn the toy-example of f0 measurements, you will want to include varying terms for participant and word. These will take care to let the model know of the structure of the data mentioned above. If you have other predictors in the model, you should also add them as (varying) slopes in the varying terms. For example: (question | participant) + (question | word) (where question = statement vs question).\nHere are some tutorials: Winter (2013), DeBruine and Barr (2021), Kirby and Sonderegger (2018), B√ºrkner (2018), Veenman, Stefan, and Haaf (2023), Pedersen et al. (2019)."
  },
  {
    "objectID": "extra/regression-sheet.html#step-3-are-there-non-linear-effects",
    "href": "extra/regression-sheet.html#step-3-are-there-non-linear-effects",
    "title": "Regression models: a cheat-sheet",
    "section": "5 Step 3: Are there non-linear effects?",
    "text": "5 Step 3: Are there non-linear effects?\nA typical use-case of non-linear terms is when you are dealing with time-series data or spatial data (i.e.¬†geographic coordinates). Generalised Additive Models (GAMs) allow you to fit non-linear effects using so called ‚Äúsmooth‚Äù (or ‚Äúsmoother‚Äù) terms. You can fit a regression model with smooth terms with brms::brm(y ~ s(x)) or with mgcv:gam(y ~ s(x)), among others. See Simpson (2018), S√≥skuthy (2017), S√≥skuthy (2021), Wieling (2018), Pedersen et al. (2019)."
  },
  {
    "objectID": "extra/regression-sheet.html#step-0-bis-number-of-outcome-variables",
    "href": "extra/regression-sheet.html#step-0-bis-number-of-outcome-variables",
    "title": "Regression models: a cheat-sheet",
    "section": "6 Step 0-bis: Number of outcome variables",
    "text": "6 Step 0-bis: Number of outcome variables\nIf you want to model just one outcome variable, you are already covered if you went through steps 1-3. If instead your design has two or more outcome variables (for example F1 and F2, or duration of the stressed and unstressed vowel of a word) which you want to model together, then you want to fit a multivariate model (i.e.¬†a model with multiple outcome variables). The same steps we went through before can be applied to multiple outcome variables. In some cases, you will want to use the same model structure for all the outcome variables, while in others you might want to use a different model structure for each.\nTo learn more about multivariate models, I really recommend B√ºrkner (2024)."
  },
  {
    "objectID": "extra/regression-sheet.html#footnotes",
    "href": "extra/regression-sheet.html#footnotes",
    "title": "Regression models: a cheat-sheet",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTechnically, the ‚Äúsimplest‚Äù linear model is \\(y = f(x)\\), but oh well‚Ä¶‚Ü©Ô∏é\nNote that, despite using family = binomial in glm(), under the hood a Bernoulli distribution is used.‚Ü©Ô∏é"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Analysis @ UoE LEL",
    "section": "",
    "text": "Want to learn about data analysis for language research?\nFor resources on research methods in linguistics, check out the Study design, Measures and Data analysis pages. For more general research skills and software, check the Skills and Software pages. To know which courses in research methods/data analysis are offered to LEL students, see the Courses page.\nWe also run the LEL STeW series (LEL Statistical Training Workshops series) which is open to both students and staff. More information can be found on the STeW page.\n\n\nNeed help with quantitative data analysis and statistics?\nIf you need help with quantitative data analysis or statistical aspects of your research or dissertation, please find the available support channels on the Support page."
  },
  {
    "objectID": "measures/phonetics.html",
    "href": "measures/phonetics.html",
    "title": "Measures and metrics",
    "section": "",
    "text": "Using Praat for linguistic research is an excellent starting point for learning which measurements and metrics can be used for which purposes and how to obtain them in Praat.\nCommon measurements are: Voice Onset Time, f0, pulses, jitter, shimmer, Harmonics-To-Noise ratio, formant frequency and bandwidth, intensity, harmonic amplitude, spectral tilt, centre of gravity, ‚Ä¶\nThis tutorial explains how to extract cepstral peak prominence (CPP) measures in Praat: A Practical Guide to Calculating Cepstral Peak Prominence in Praat, https://doi.org/10.31219/osf.io/yvp4s."
  },
  {
    "objectID": "measures/phonetics.html#acoustics",
    "href": "measures/phonetics.html#acoustics",
    "title": "Measures and metrics",
    "section": "",
    "text": "Using Praat for linguistic research is an excellent starting point for learning which measurements and metrics can be used for which purposes and how to obtain them in Praat.\nCommon measurements are: Voice Onset Time, f0, pulses, jitter, shimmer, Harmonics-To-Noise ratio, formant frequency and bandwidth, intensity, harmonic amplitude, spectral tilt, centre of gravity, ‚Ä¶\nThis tutorial explains how to extract cepstral peak prominence (CPP) measures in Praat: A Practical Guide to Calculating Cepstral Peak Prominence in Praat, https://doi.org/10.31219/osf.io/yvp4s."
  },
  {
    "objectID": "measures/phonetics.html#perceptual-data",
    "href": "measures/phonetics.html#perceptual-data",
    "title": "Measures and metrics",
    "section": "2 Perceptual data",
    "text": "2 Perceptual data\nThis tutorial by Joseph Casillas explains how to analyse categorisation data using linear models.\nIn particular, the tutorial shows how to obtain an estimate of the category boundary and the sharpness of the boundary."
  },
  {
    "objectID": "measures/phonetics.html#articulatory-data",
    "href": "measures/phonetics.html#articulatory-data",
    "title": "Measures and metrics",
    "section": "3 Articulatory data",
    "text": "3 Articulatory data\n‚Ä¶"
  },
  {
    "objectID": "measures/phonetics.html#recent-developments",
    "href": "measures/phonetics.html#recent-developments",
    "title": "Measures and metrics",
    "section": "4 Recent developments",
    "text": "4 Recent developments\n\nNasality from Acoustic Features (NAF): Chris Carignan. 2021. A practical method of estimating the time-varying degree of vowel nasalization from acoustic features https://doi.org/10.1121/10.0002925."
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "ELAN: annotation tool for audio and video recordings.\nSIL Field linguist‚Äôs Toolbox: Toolbox is a data management and analysis tool for field linguists. It is especially useful for maintaining lexical data, and for parsing and interlinearizing text, but it can be used to manage virtually any kind of data.\nSIL FieldWorks: FieldWorks consists of software tools that help you manage linguistic and cultural data.\nSIL LexiquePro: Create and manage dictionaries.\nSIL miscellanea software."
  },
  {
    "objectID": "software.html#language-data-management",
    "href": "software.html#language-data-management",
    "title": "Software",
    "section": "",
    "text": "ELAN: annotation tool for audio and video recordings.\nSIL Field linguist‚Äôs Toolbox: Toolbox is a data management and analysis tool for field linguists. It is especially useful for maintaining lexical data, and for parsing and interlinearizing text, but it can be used to manage virtually any kind of data.\nSIL FieldWorks: FieldWorks consists of software tools that help you manage linguistic and cultural data.\nSIL LexiquePro: Create and manage dictionaries.\nSIL miscellanea software."
  },
  {
    "objectID": "software.html#corpus-research",
    "href": "software.html#corpus-research",
    "title": "Software",
    "section": "2 Corpus research",
    "text": "2 Corpus research\n\nAntConc: A freeware corpus analysis toolkit for concordancing and text analysis.\nManSeeks: Simple yet fast concordancer in an Electron app.\nMultidimensional Analysis Tagger: The Multidimensional Analysis Tagger (MAT) is a program that replicates Biber‚Äôs (1988) Variation across Speech and Writing tagger for the multidimensional functional analysis of English texts, generally applied for studies on text type or genre variation.\nSketchEngine [license needed]."
  },
  {
    "objectID": "software.html#qualitative-analysis",
    "href": "software.html#qualitative-analysis",
    "title": "Software",
    "section": "3 Qualitative analysis",
    "text": "3 Qualitative analysis\n\nNVivo [license needed].\nRQDA R package.\nTaguette."
  },
  {
    "objectID": "software.html#speech-analysis-and-audio-processing",
    "href": "software.html#speech-analysis-and-audio-processing",
    "title": "Software",
    "section": "4 Speech analysis and audio processing",
    "text": "4 Speech analysis and audio processing\n\nArticulate Assistant Advanced: Articulate Assistant Advanced (AAA) is software designed to record and analyse speech production data from instruments such as ultrasound, video, EMA, VICON, MRI and EPG [license needed].\nAudacity: free, open source, cross-platform audio software.\nDeepLabCut for speech production.\nEMU Speech Database Management System.\nPraat: acoustic analysis software.\n\nExcellent guidebook by Will Styler: https://github.com/stylerw/usingpraat.\nPraat scripting tutorial by Dr.¬†J√∂rg Mayer: https://praatscripting.lingphon.net.\n\nWaveSurfer: WaveSurfer is an open source tool for sound visualization and manipulation.\n\n\n4.0.1 Forced-aligners\n\nBAS Web Services.\nSPeech Phonetization Alignment and Syllabification (SPPAS).\nFAVE: FAVE-align and FAVE-extract tools.\nMontreal Forced Aligner."
  },
  {
    "objectID": "software.html#phonology",
    "href": "software.html#phonology",
    "title": "Software",
    "section": "5 Phonology",
    "text": "5 Phonology\n\nExperiGen.\nOTSoft: OTSoft is a Windows program meant to facilitate analysis in Optimality Theory and related frameworks by using algorithms to do tasks that are too large or complex to be done reliably by hand.\nPhonology Assistant: it automatically charts the sounds and through its searching capabilities, helps a user discover and test the rules of sound in a language.\nPhonoApps contains computational and learning tools for phonologists."
  },
  {
    "objectID": "software.html#behavioural-experiments",
    "href": "software.html#behavioural-experiments",
    "title": "Software",
    "section": "6 Behavioural experiments",
    "text": "6 Behavioural experiments\n\nGorilla Experiment Builder [license needed].\nProlific: Quickly find research participants you can trust [paid service].\nPsychJS: Build online experiments with JavaScript. A tutorial by Kenny Smith.\nPsychoPy: PsychoPy is a free cross-platform package allowing you to run a wide range of experiments in the behavioural sciences.\nQualtrics: Survey platform [license needed].\nTestable: Solution for behavioural experiments, surveys and data collection [free and licensed options]."
  },
  {
    "objectID": "software.html#miscellanea",
    "href": "software.html#miscellanea",
    "title": "Software",
    "section": "7 Miscellanea",
    "text": "7 Miscellanea\n\nQGIS: A Free and Open Source Geographic Information System.\nSplits Tree: computing phylogenetic trees."
  },
  {
    "objectID": "software.html#typefaces-and-input-methods",
    "href": "software.html#typefaces-and-input-methods",
    "title": "Software",
    "section": "8 Typefaces and input methods",
    "text": "8 Typefaces and input methods\n\n8.1 Serif\n\nBrill typeface.\nConstructium.\nHeuristica.\nJunicode.\nLinguistics Pro.\nTinos.\n\n\n\n8.2 Sans-serif\n\nAtkinson Hyperlegible.\nVoces.\n\n\n\n8.3 Mono-space\n\nIosevka.\n\n\n\n8.4 Bundles\n\nDejaVu.\nFira fonts: Fira Sans, Fira Code, Fira Mono.\nGNU FreeFont.\nNoto.\n\n\n\n8.5 Input methods\n\nIPA Palette: IPA Input Method for Mac OS X 10.6 and later.\nSIL IPA Unicode keybords."
  },
  {
    "objectID": "study-design.html",
    "href": "study-design.html",
    "title": "Study design",
    "section": "",
    "text": "Mits Ota has created a slide deck on how to plan a study for your research project. You can find the slides on research questions and hypotheses here.\nThe online course Constructing a hypothesis, from the PPLS Practical Research Skills website, teaches you ‚Äúhow to operationalise your research question into an informative and falsifiable hypothesis‚Äù."
  },
  {
    "objectID": "study-design.html#research-questions-and-hypotheses",
    "href": "study-design.html#research-questions-and-hypotheses",
    "title": "Study design",
    "section": "",
    "text": "Mits Ota has created a slide deck on how to plan a study for your research project. You can find the slides on research questions and hypotheses here.\nThe online course Constructing a hypothesis, from the PPLS Practical Research Skills website, teaches you ‚Äúhow to operationalise your research question into an informative and falsifiable hypothesis‚Äù."
  },
  {
    "objectID": "study-design.html#experimental-designs",
    "href": "study-design.html#experimental-designs",
    "title": "Study design",
    "section": "2 Experimental designs",
    "text": "2 Experimental designs\nYou can find the slides on experimental designs here."
  },
  {
    "objectID": "study-design.html#research-ethics",
    "href": "study-design.html#research-ethics",
    "title": "Study design",
    "section": "3 Research Ethics",
    "text": "3 Research Ethics\nCheck out the Linguistics and English Language Research Ethics Information website."
  }
]